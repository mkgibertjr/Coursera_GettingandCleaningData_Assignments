---
title: 'Getting and Cleaning Data - Assignment'
author: "Myron Keith Gibert Jr"
date: "March 31, 2020"
output: pdf_document
toc: true
linkcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r tinytex, include=FALSE}
## INSTALLING LATEX FOR RMARKDOWN

#RMarkdown requires LaTex to create pdf documents as an output. More information can be found [here](https://bookdown.org/yihui/rmarkdown/pdf-document.html). Alternatively, output can be set to "html_document" or "word_document". End users may install LaTex by setting the "wanttinytex" variable to FALSE and running the following script:

#Install tinytex to let RMarkdown create a pdf document? Default: wanttinytex <- FALSE
wanttinytex <- FALSE

if(tinytex:::is_tinytex()==FALSE && wanttinytex == TRUE){
if (!require("tinytex")) install.packages("tinytex")
tinytex::install_tinytex()
}
```

## Correspondence 
Please address any questions to Myron Keith Gibert Jr at [mkgibertjr@msn.com](mailto:mkgibertjr@msn.com).  Code for this project is stored in a [GitHub repository](https://github.com/mkgibertjr/Coursera_GettingandCleaningData_Assignments).

## Introduction
The purpose of this project is to demonstrate my ability to collect, work with, and clean a data set. The goal is to prepare tidy data that can be used for later analysis. I was graded by my peers on a series of yes/no questions related to the project. I was required to submit: 1) a tidy data set as described below, 2) a link to a Github repository with my script for performing the analysis, and 3) a code book that describes the variables, the data, and any transformations or work that I performed to clean up the data called CodeBook.md. I also included a README.md in the Github repository (repo) with my scripts. This repo explains how all of the scripts work and how they are connected.

One of the most exciting areas in all of data science right now is wearable computing - see for example [this article](http://www.insideactivitytracking.com/data-science-activity-tracking-and-the-battle-for-the-worlds-top-sports-brand/) . Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data linked to from the course website represent data collected from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained:

[http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones](http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)

I created an R script called run_analysis.R that does the following.

1. Merges the training and the test sets to create one data set.
2. Extracts only the measurements on the mean and standard deviation for each measurement.
3. Uses descriptive activity names to name the activities in the data set
4. Appropriately labels the data set with descriptive variable names.
5. From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.

## Set Parameters
```{r parameters}
#set the directory that contains the files
path <- "UCI HAR Dataset"
#identifier 1
id1 <- 1
#identifier 2
id2 <- 2
#separator
separator <- "_"
#set output directory? Default: outputdir <- "assignment1outputs"
outputdir <- "assignmentoutputs"
#Overwrite contents of the output directory? Default: deleteoutputs <- FALSE
deleteoutputs <- TRUE
#Delete specdata/ directory after completing the analysis? Default: deletespec <- TRUE
deletespec <- TRUE

```

## Debug

The debug chunk will prevent the script from running if any of the dependent variables for this analysis do not exist.  This should prevent the program from erroring out after a long runtime without producing any results due to a missing variable.  If modifying the input .csv and .xlsx files, it is important to leave all header information and column names intact, as the program uses this information to extract relevant data.  Columns are intuitively labeled to end user convenience.

```{r debug}
if (dir.exists(outputdir) && deleteoutputs == FALSE ){
  stop("Your output directory already exists!  Please delete/move 
       this folder from your working directory.  Alternatively, you 
       can set 'deleteoutputs' to TRUE to auto-delete this folder 
       for every run. You may also choose an alternative output 
       directory.")
}else{
  unlink(outputdir,recursive = TRUE)
}

if (!exists("outputdir")){
stop("outputdir variable is not defined.  Please ensure that all 
     parameters in the r parameters chunk are defined.")
}

if (!exists("deleteoutputs")){
stop("deleteoutputs variable is not defined.  Please ensure that all 
     parameters in the r parameters chunk are defined.")
}

if (!exists("deletespec")){
stop("deletespec variable is not defined.  Please ensure that all 
     parameters in the r parameters chunk are defined.")
}

if (!dir.exists(outputdir)){dir.create(outputdir)}
```

## Data
The zip file containing the data for this assignment can be downloaded here:

[https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip](https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip)

I have renamed the file to "ASN_rcleaning_data.zip" for organization.

## Unzipping the data

For this programming assignment I needed to unzip this file and create the directory 'ASN_rcleaning_data'. Once I unzipped the zip file, I did not make any modifications to the files in the 'ASN_rcleaning_data' directory. 

```{r unzip}
if(!dir.exists("UCI HAR Dataset")){
  unzip("ASN_rcleaning_data.zip")
}

```

## Reading in the data

```{r data}

i = 1

filenames.uci <-  list.files(path = path,pattern = "\\.txt$")

for(i in 1:length(filenames.uci)){
  tryCatch({
    print(filenames.uci[i])
    varname <- gsub('.txt', '', paste(filenames.uci[i]))
    datafile <- paste(path,filenames.uci[i],sep="/")
    assign(paste(varname), read.table(datafile))
  }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}
rm(varname)
rm(datafile)

filenames.test <- list.files(path = paste(path,"test",sep="/"),pattern = "\\.txt$")

for(i in 1:length(filenames.test)){
  tryCatch({
    print(filenames.test[i])
    varname <- gsub('.txt', '', paste(filenames.test[i]))
    datafile <- paste(path,"test",filenames.test[i],sep="/")
    assign(paste(varname), read.table(datafile))
  }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}
rm(varname)
rm(datafile)

filenames.train <- list.files(path = paste(path,"train",sep="/"),pattern = "\\.txt$")

for(i in 1:length(filenames.train)){
  tryCatch({
    print(filenames.train[i])
    varname <- gsub('.txt', '', paste(filenames.train[i]))
    datafile <- paste(path,"train",filenames.train[i],sep="/")
    assign(paste(varname), read.table(datafile))
  }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}
rm(varname)
rm(datafile)

```

## Step 1: Merge training and test sets to create one data set.

```{r step1}

listenvironment <- ls()
pattern <- "_train|_test"

object.find <- grep(pattern,listenvironment)
objectnames <- listenvironment[object.find]
object.categories <- t(as.data.frame(str_split(objectnames,separator)))

identifier1 <- unique(object.categories[,id1])
identifier2 <- unique(object.categories[,id2])

i=1
for (i in 1:length(identifier1)){
    print(identifier1[i])
    combineR <- data.frame()
    h = 1
  for(h in 1:length(identifier2)){
    dataname <- as.name(paste(identifier1[i],identifier2[h],sep=separator))
    combineR <- rbind(combineR,eval(parse(text = dataname)))
  }
    assign(identifier1[i],combineR)
}

```

## Step 2: Extract only the mean and standard deviation for each measurement

```{r step2}

MeanStdOnly <- grep("mean()|std()", features[, 2]) 
dataSet <- X[,MeanStdOnly]

```

## Step 3: Use descriptive activity names to name the activities in the data set

```{r step3, eval = FALSE}

# group the activity column of dataSet, re-name lable of levels with activity_levels, and apply it to dataSet.
act_group <- factor(dataSet$activity)
levels(act_group) <- activity_labels[,2]
dataSet$activity <- act_group

```

## Step 4: Appropriately labels the data set with descriptive activity names

```{r step4, eval = FALSE}

# Create vector of "Clean" feature names by getting rid of "()" apply to the dataSet to rename labels.
CleanFeatureNames <- sapply(features[, 2], function(x) {gsub("[()]", "",x)})
names(dataSet) <- CleanFeatureNames[MeanStdOnly]

# combine test and train of subject data and activity data, give descriptive lables
subject <- rbind(subject_train, subject_test)
names(subject) <- 'subject'
activity <- rbind(y_train, y_test)
names(activity) <- 'activity'

# combine subject, activity, and mean and std only data set to create final data set.
dataSet <- cbind(subject,activity, dataSet)

```


## Step 5: Creates a second, tidy data set with the average of each variable for each activity and each subject.

```{r step5, eval = FALSE}

# check if reshape2 package is installed
if (!"reshape2" %in% installed.packages()) {
	install.packages("reshape2")
}
library("reshape2")

# melt data to tall skinny data and cast means. Finally write the tidy data to the working directory as "tidy_data.txt"
baseData <- melt(dataSet,(id.vars=c("subject","activity")))
secondDataSet <- dcast(baseData, subject + activity ~ variable, mean)
names(secondDataSet)[-c(1:2)] <- paste("[mean of]" , names(secondDataSet)[-c(1:2)] )
write.table(secondDataSet, "tidy_data.txt", sep = ",")

```


## Cleanup
This final command removes the unzipped "UCI HAR Dataset" directory if the deletespec variable is set to TRUE.  This reduces the overall storage burden of this project by removing the files that we no longer need access to. The zipped file remains in the working directory, so "UCI HAR Dataset" will be recreated whenever I use the command in line 100 (unzip) if it is deleted during this step.

```{r cleanup}
if(deletespec == TRUE){unlink("UCI HAR Dataset",recursive = TRUE)}
```

